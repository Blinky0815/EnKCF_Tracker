\documentclass{bmvc2k}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools} % mathtools builds on and extends amsmath package
\usepackage{algorithm}		% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}	% http://ctan.org/pkg/algorithmicx% Include other packages here, before hyperref.
\usepackage{comment, url}
%% Enter your paper number here for the review copy
\bmvcreviewcopy{825}

\title{ {\it E}nKCF: Ensemble of Kernelized Correlation Filters for Object Tracking in High Speed}

% Enter the paper's authors in order
% \addauthor{Name}{email/homepage}{INSTITUTION_CODE}
\addauthor{Susan Student}{http://www.vision.inst.ac.uk/~ss}{1}
\addauthor{Petra Prof}{http://www.vision.inst.ac.uk/~pp}{1}
\addauthor{Colin Collaborator}{colin@collaborators.com}{2}

% Enter the institutions
% \addinstitution{Name\\Address}
\addinstitution{
 The Vision Institute\\
 University of Borsetshire\\
 Wimbleham, UK
}
\addinstitution{
 Collaborators, Inc.\\
 123 Park Avenue,\\
 New York, USA
}

\runninghead{Student, Prof, Collaborator}{BMVC Author Guidelines}

% Any macro definitions you would like to include
% These are not defined in the style file, because they don't begin
% with \bmva, so they might conflict with the user's own macros.
% The \bmvaOneDot macro adds a full stop unless there is one in the
% text already.
\def\eg{\emph{e.g}\bmvaOneDot}
\def\Eg{\emph{E.g}\bmvaOneDot}
\def\etal{\emph{et al}\bmvaOneDot}

%-------------------------------------------------------------------------
% Document starts here
\begin{document}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Computer vision technologies are very attractive for practical
applications deployed on embedded systems. This is primarily because
most of embedded systems already have image acquisition pipeline and a
tremendous amount of progresses on research has been made for many
areas in computer vision. However, to successfully deploy a computer
vision algorithm on any existing embedded systems, a vision algorithm
needs to satisfy, at least, two criteria: minimal, manual intervention
after deployment and small footprint of consuming computational
resources and on executable code, with the assumption of reasonably
good performance. To this end, in this paper, we propose an ensemble
of the kernelized correlation filters (KCF), we call {\it E}nKCF, for
the problem of a single-target object tracking. In particular, we
developed a committee of KCFs to specifically address the scale-change
and dynamic maneuver of the target over frames. In addition, we
developed a Bayes filter for a smooth transition between individual
KCFs' executions. Experimental results showed that the performance of
ours are 70.10\% for precision at 20 pixels and 53.00\% for success
rate for OTB100 data, and 54.50\% and 40.2\% for UAV123 data. These
results showed that our method is better than existing ones over 5\%
on precision on 20 pixels and 10-20\% on AUC on average. Moreover our
implementation ran at 340 fps for OTB100 and at 416 fps for UAV123
data that is faster than DCF (292 fps) for OTB100 and KCF (292 fps),
DCF (457 fps) for UAV123.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
A recent proliferation of air/ground/water unmanned vehicle
technologies has ever increased interests on deploying intelligent
algorithms to any existing embedded and/or mobile platforms. Among
those technologies, computer vision algorithms are getting more
attentions primarily because payloads of those mobile platforms are
limited to carry any other sensors than a lightweight or an array of
monocular cameras. For example, instead of having just bare flight
function with video recording, an unmanned air vehicle (UAV) equipped
with an object or feature following functionality would make it more
useful in the application of monitoring/surveillance/surveying on
private properties/wild-life/crop, video recording on
sports/movies/events, many others.

In this paper, we propose a single-target tracking algorithm aiming at
running on any embedded or mobile platforms that doesn't require an
offline training, and can run on an embedded system. Specifically, we
would like to make our algorithm 1) learn the appearance model of a
target on the fly and 2) run as fast on a typical desktop as up to
$300$-$400$Hz so that it could run, at least, faster than 30Hz when it
is deployed on a low-end, embedded system.\footnote{We empirically
  found this computation reasonable -- the computation about an 1/10th
  scale down of computational resources from a typical PC to an
  embedded system.}

One of the dominant frameworks for online object tracking is the
correlation filter that essentially solves a single-target tracking
problem as a regression problem in the frequency domain. This
framework assumes that the target location is given at the very first
image frame like other online tracking algorithms
\cite{smeulders2014survey}. Given this positive example for the
regression problem, a set of negative examples is collected around the
initial, target bounding box and represented as a circulant matrix
\cite{henriques2015high}. One can optimally solve this regression
problem using a ridge regression in a closed form. However, this
solution involves in expensive matrix operations
$\mathcal{O}(n^{3})$. The correlation filter offers a less complex
solution, $\mathcal{O}(n\log n)$ over element-wise multiplication in a
frequency domain \cite{bolme2010visual,henriques2015high}. Thank to
this reformulation, an object tracking pipeline based on the
correlation filter can be easily implemented and run very efficiently
in an online fashion. In fact, a variant of the correlation filter,
the kernelized correlation filter with multi-channel features
\cite{henriques2015high} showed impressive object tracking results and
outperformed other state-of-the-art tracking algorithms, for example,
in VOT15 challenge, in terms of run-time and tracking
accuracy. However, a vanilla form of such an online tracker is prone
to drift, and fails to track a target over a long period of time
primarily \cite{henriques2015high} because of the dilemma of
stability-plasticity in updating appearance model, where the
appearance model will be overfitted to only the images used to train
and a compromise on the frequency of updating the model is carefully
implemented \cite{santner2010prost}. For example, a naive way of
handling a scale change of a target by just having multiple
correlation filters running on each frame could reduce the run-time
performance from 300 to 100 fps. An intuitive way of addressing the
scale change is to scan the region of interest (ROI) with templates in
different dimensions based on pre-defined scale ratios to find the
target
\cite{henriques2015high,tang2015multi,ma2015long,bibi2015multi,li2014scale}. Due
to the computational nature of correlation filter, however, this
approach would merely increase the computational complexity because it
has to run the correlation filter multiple times on each frame.

\begin{figure*}[!t]
\includegraphics[width=\textwidth]{figures/ResultsIntroduction.pdf}
\caption{Examples of tracking results (yellow rectangles) by the
  proposed method on the ``UAV123'' dataset. This ``UAV123'' dataset
  is challenging for object tracking as a target's scale is changed
  drastically in a few frames.}
\label{ResultsIntroduction}
\end{figure*}

Another way of handling the scale change for the correlation filter
based approach is to estimate a correct scale at a location where a
target highly likely appears \cite{zhang2014fast} -- estimate a
target's translation first and estimate a correct scale afterward. In
particular, they use the MOSSE tracker to estimate a target's
translation. And then they attemped to update the scale of the target
a way where the confidence map is used to determine the scale change
between successive frames. This is based on an assumption that the
scale of a target wouldn't change much over two consecutive
frames. Similarly \cite{ma2015long} used two KCFs to learn the
translation and scale of a target separately. To be more specific, a
KCF is used to learn the translation of the target and its
background. Given this, another KCF is used to learn the target area
to estimate a new scale of the target. However, because of running
more than a KCF on each frame, this method degrades its run-time
performance (i.e., $\leq 50 fps$). Our method is motivated by this
idea -- the idea of deploying multiple KCFs to address the issues of
single target tracking: scale and translation, but in a more efficient
way. To be more specific, we use an ensemble of KCFs in turn, instead
of running them all together on every frame. In particular, we deploy
three KCF in turn: \textit{target}+\textit{small background}
translation filter ($R_{t}^{S}$), \textit{target-only} scale filter
($R_{s}$) and \textit{target}+\textit{large background} translation
filter ($R_{t}^{L}$). By doing so, we could still address the scale
change and estimate target's motion efficiently while increasing
run-time performance. Figure \ref{fig:Filters} shows examples of the
region of interest (ROI) associated with $R_{t}^{S}$, $R_{t}^{L}$, and
$R_{s}$. Figure \ref{Workflows} illustrates the work-flow of the {\it
  E}nKCF.
\begin{figure}[!t]
\centering
\begin{tabular}{cc}
\bmvaHangBox{\includegraphics[width=12.00cm]{./figures//Workflow_MKCF+PF.pdf}}\\
\end{tabular}
\caption{The workflow of {\it E}nKCF tracking algorithm.}
\label{Workflows}
\end{figure}
The contribution of this study is a novel, single-target tracking
algorithm running in a very high-speed, $\geq 300$ fps. In particular,
the proposed algorithm, {\it E}nKCF, deploys an ensemble of KCF, in
turn, to more effectively address the scale variance and the dynamic
maneuver of a target while preserving run-time performance as high as
possible. To minimize any potential performance gap from transiting
one KCF to another, we use a Bayes filter.

%---------------------------------------------------------------------- 
\section{{\it E}nKCF: Ensemble of Kernelized Correlation Filters}
%---------------------------------------------------------------------- 
In this paper, we extend the KCF in that three KCFs are deployed, in
turn, to address a target's scale change and dynamic maneuvers while
preserving the overall execution time very fast, i.e., $\ge$ 300
fps. The efficient nature of the KCF computation results in a
small-footprint, executable. Our design principle is, if we could
deploy these KCFs in an efficient way, that the proposed algorithm
could achieve a high, run-time performance while effectively
addressing the scale variation and dynamic maneuvers of a target.

The proposed algorithm, {\it E}nKCF, learns three KCFs in turn: The
first filter, $R_{t}^{S}$, focuses on learning the target area and its
background, \textit{target}+\textit{small background} for addressing a
marginal translation by a target, the second filter, $R_{s}$, focuses
entirely on learning the target's scale, \textit{target-only}, and the
last filter, $R_{t}^{L}$, focuses on the target area and its
background twice bigger than that of the first filter, $R_{t}^{S}$,
\textit{target}+\textit{large background}. We set up {\it E}nKCF in
this way because we believe a correlation filter for learning a
target's translation should include its background to better
discriminate the target from its background and another correlation
filter is prepared to focus on the target itself to estimate the right
size or scale of the target. Figkure \ref{fig:Filters} shows examples
of the ROI covered by each of three filters. We believe that a filter
for learning a target's translation should include its background. In
addition, making the ROI for the scale filter, $R_{s}$, tighter
reduces the size of the template for the element-wise multiplication,
resulting in a scale estimation in a higher speed.

\begin{figure}[!h]
\centering
\includegraphics[width=0.45\textwidth]{figures/Filters_Details.pdf}
\caption{Examples of three filters; the translation filter,
  \textit{target+small background}, the scale filter,
  \textit{target-only} and another translation filter,
  \textit{target+large background}, and their Hanning window and
  expected Gaussian response.}
\label{fig:Filters}
\end{figure}

After Henriques and his colleagues' presented impressive tracking
results of KCF \cite{henriques2015high} on VOT15
challenge \footnote{\url{http://www.votchallenge.net/vot2015/}}, many
variation of KCF were investigated. In particular, \cite{ma2015long}
and \cite{danelljan2014accurate} used a scale filter to learn the
target area only whereas \cite{li2014scale, bibi2015multi,
  tang2015multi} used a correlation filter learned on the target area
and its surrounding background in different size to learn the optimal
scale of the target. Our approach is particularly similar to that of
\cite{ma2015long} in that more than one KCF is used to address the
challenges of single target tracking. But ours is different from that
of \cite{ma2015long} in that we do not use those multiple KCFs
together at every frame, but alternatively at every three frame. The
way our algorithm is alternating multiple KCFs in different goals
intuitively makes sense in that the appearance of consecutive images
does not change drastically. Although it varies based on the scene,
learning a correlation filter a frame would not be much different from
the one learned using the image few or $k$ frames later. 
\begin{comment}
Figure
\ref{fig:Filters_Comparison} shows examples supporting this
observation.
\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figures/LearnedFilterComparison.pdf}
\caption{This figure shows that there is marginal difference among the
  scale filters learned either at every frame or at every 5
  frames. The sub-figures at the top row show the scale filters
  trained from every frame and those at the bottom row show the scale
  filters estimated every 5 frames.}
\label{fig:Filters_Comparison}
\end{figure}
\end{comment}
The order of running these three KCFs is important because each of
these filters aims at addressing different challenges; The
\textit{target+large background} translation filter, $R_{t}^{L}$, is
applied to the $i$th and $i+1$th image frame, another translation
filter, $R_{t}^{S}$ is applied to the $i+2$th and $i+3$th image frame,
and then the scale filter, $R_{s}$ is applied to the $i+4$the
image. This order repeats until the end of the image frames. The
translation filter, $R_{t}^{L}$, is intended to run right after the
scale filter, $R_s$, runs which is applied at every other $i+4$
frames. We run these filters in this way because we want to minimize
any drifts that are likely to happen running only $R_{s}$. In
addition, the filter, $R_{t}^{S}$, is applied to every other two
frames before $R_{s}$ and right after two consecutive frames running
$R_{t}^{L}$. By repeating this order of learning KCFs, we can
integrate more discriminative shape features that cannot be learned by
$R_{t}^{L}$ because it incorporates a large area about background in
the learned model. The filter, $R_{t}^{L}$, uses shape and color
information together to recover from any potential drifts -- drifts
could happen only looking the target's scale by the filter, $R_{s}$.
\begin{comment}
In summary, the scale filter, $R_{s}$, is designed to directly handle
the scale change of a target and provides $R_{t}^{L}$ and $R_{t}^{S}$
with more focused ROIs about the target's translation. A translation
filter, $R_{t}^{L}$, is intended to look at a larger search area to
estimate the target's translation and recover from any potential
drifts. Another translation filter, $R_{t}^{S}$, on the other hand, is
designed to addresses the drawback of $R_{t}^{L}$ that it may learn
noisy shape features due to its large search area.
\end{comment}
Figure \ref{Workflows} illustrates the work-flow of the proposed
 algorithm and alg.\ref{alg:MKCF} shows the pseudo-code of {\it
   E}nKCF. In the following section, we will briefly mention the
 theory behind the KCF.
\begin{algorithm}[h]
\small
	\caption{{\it E}nKCF Tracking Algorithm.}\label{alg:MKCF}
	\begin{algorithmic}[1]
	\State \textbf{Input} : Initial bounding box $x_{0}$, frame counter $fc$, scale filter frequency $n = 5$,
	\State \textbf{Output} : 
				\If{$fc\:\%\:n=0$} \Comment{Condition 1}
						\State Estimated Target State $x_{t} = (x_{t},y_{t},s_{t})$,
						Scale filter (\textit{target-only}) model $R_{s}$.
			     \EndIf
				\If{$fc\:\%\:n=1\:\:or\:\:fc\:\%\:n=2$}\Comment{Condition 2}
						\State Estimated Target State $x_{t} = (x_{t},y_{t},s_{t_1})$,
						Large Area Translation Filter model $R_{t}^{L}$.
				\EndIf
				\If{$fc\:\%\:n=3\:\:or\:\:fc\:\%\:n=4$}\Comment{Condition 3}
						\State Estimated Target State $x_{t} = (x_{t},y_{t},s_{t})$,
						Small Area Translation Filter model $R_{t}^{S}$.
				\EndIf
	\Procedure{track}{$x_{t-1},y_{t-1},s_{t-1}$} 
				\State // Translation Estimation
				\State Transit Particle Filter to the frame $t$ and compute the mean of prior pdf $x_{t} = (x_{t},y_{t},s_{t-1})$;
				\State // Translation Estimation
				\State Crop the ROI for the $R_{t}^{L}$, or $R_{t}^{S}$ given $x_{t}$
				and estimate translation ($x_{t}$,$y_{t}$) using $R_{t}^{L}$ (Condition 2) or $R_{t}^{S}$ (Condition 3),
				\State Skip translation estimation for $R_{s}$ (Condition 1);
				\State // Scale Estimation
			    \State Crop the ROI for the $R_{s}$ and estimate scale, $s_{t}$, using $R_{s}$ (Condition 1), 
				\State Scale pool for $R_{s}$ : $\lbrace1.05,1.0,1/1.05\rbrace$;
		        \State Skip it for $R_{t}^{L}$ and $R_{t}^{S}$ (Condition 1, and 2),
				\State // Update Translation
				\State Perform Importance Re-sampling (if necessary) for Particle Filter and compute the mean of posterior pdf $x_{t} = (x_{t},y_{t},s_{t})$;
			    \State // Model Update
			    \State Update $R_{s}$ (Condition 1);
			    \State Update $R_{t}^{L}$ (Condition 2);
			     \If{$PSR(y_{R_{t}^{S}}) \geq T_{R_{t}^{S}}$}
				\State Update $R_{t}^{S}$ (Condition 3);
			     \EndIf		
	\EndProcedure	
	\end{algorithmic}
\end{algorithm}

\subsection{Kernelized Correlation Filter} \label{sec:kcf}
The Kernelized Correlation Filter is a well, known single target
tracking method and thetheory and its workflowhas been detailed in
other papers already
\cite{henriques2012exploiting,henriques2015high}. This section briefly
goes over the parts of the KCF relevantto thisstudy. Its
computational efficiency arises from its use of the discrete Fourier
transform of the circulant matrix and frequency domain. The KCF
essentially solves the problem of a regression in the form of the
regularized ridge regression as:
\begin{equation}
E(h) = \frac{1}{2}||y-\sum_{c=1}^{C}h_{c}*x_{c}||^{2} + \frac{\lambda}{2}\sum_{c=1}^{C}||h_{c}||^{2}
\label{eq:Closedform_RidgeReg}
\end{equation}
where $y$ represents the desired continuous response whereas $h$ and
$x_{c}$ represents the learned correlation filter and training
template for the given channel. The parameter $c$ enabled one to
integrate features in multiple channels, such as HoG and color, in
this setup \cite{henriques2015high,galoogahi2013multi}. A closed-form
solution for Equaiton \ref{eq:Closedform_RidgeReg} can be computed by
taking the derivative of $E$ w.r.t $w$ to $0$. To make this,
$\mathcal{0}(n^{3})$ solution more tractable, an element-wise,
multiplication in frequency domain was proposed as a sub-linear
solution for $\hat{w}$:
\begin{equation}
\hat{w} = \hat{x}^{*}*\hat{y}(\hat{x}^{*}*\hat{x}+\lambda)^{-1}.
\label{eq:DiagonalizedPrimalSolution}
\end{equation}
In fact, a non-linear version of this closed-form solution is proposed
to make it more robust to any geometric and photometric variations
\cite{henriques2015high}. In particular, the diagonalized Fourier
domain dual form solution is expressed as
\begin{equation}
\hat{\alpha} = \hat{y}(\hat{k}^{xx}+\lambda)^{-1}
\label{eq:FourierDualDomainSolution}
\end{equation}
where $\hat{k}^{xx}$ represents the first row of the Kernel matrix $K$
known as \textit{gram matrix} and it is expressed as
\begin{equation}
k^{xx^{'}} = exp(-\dfrac{1}{\alpha^{2}}(||x||^{2}+||x^{'}||^{2}-2F^{-1}(\sum^{C}_{c}\hat{x}_{c}^{*}\odot \hat{x}_{c}^{'})))
\label{eq:GaussianCorrelationSingleChannel}
\end{equation}
An early version based on this formulation used grayscale feature
($C=1$) to learn the solution vector $w$ and multi-channel features
such as HoG and Color showed improved accuracy
\cite{henriques2015high,galoogahi2013multi,tang2015multi,ma2015long,bibi2015multi}. Specifically,
one can execute an object detection using this kernelized correlation
filter with multi-channel features as 
\begin{equation}
r(z) = F^{-1}(\hat{k}^{xz} \odot \hat{\alpha})
\end{equation}
where $r$ denotes the correlation response at all cyclic shifts of the
first row of the kernel matrix.

\subsection{Particle Filter for Smoothing Transition among KCFs} \label{sc:PF}
As explained earlier, the {\it E}nKCF updates a target's translation
at every other $k$ frames, particularly for estimating the target's
scale. Although the strategy of updating every other $k$th frames will
result in an optimal run-time performance, this may lead any potential
drifts at the later frames. To prevent such potential mishaps from
happening, we developed a Bayes filter that incorporates a target's
motion to smooth any outputs from the {\it E}nKCF. In particular, we
use a particle filter to estimate the target's image coordinate based
on the {\it E}nKCF's outputs. A particle filter is a sequential Monte
Carlo method that uses a finite number of particles or samples to
estimate the posterior probability density of a state of the system
\cite{thrun2005probabilistic}. In this study, the state represents a
target's pixel coordinates and its motion. In particular, the state of
the particle filter, $X_t$, is represented by $\lbrace x, y, v_{x},
v_{y} \rbrace$, where $x$ and $y$ are the pixel coordinates of the
target's centroid, $v_x$ and $v_y$ are the estimated velocity along
the $x$-axis and $y$-axis. The particle filter predicts, using a
constant velocity motion model, a target's state by generating a
predefined number of particles. And then the particle filter uses the
confidence maps of {\it E}nKCF as observation to update the state. The
weight of a particle is computed as, $w_{p_{t}}(x_{t},y_{t}) =
\sum_{i=1}^{N}\sum_{j=1}^{N} y_{R}(x_{t}-i,y_{t}-j)$,  where $w_{p}$
denotes the weight of the particle $p$ at time $t$ and $N$ denotes the
size of the window as that of a confidence map. Alternatively one can
also use the Euclidean distance of particles to the location of
maximum response of the confidence map. This alternative, however,
would result in drifts.
\begin{comment}
We chooses some particles based on their importance to estimate the
target's next state, i.e., its pixel coordinates as $\hat{X}_{t} =
\sum_{p=1}^{P}w_{p_{t}} X_{t}.$ Note that, unlike a typical operation
of a particle filter, we skip the importance re-sampling step when the
scale filter, $R_{s}$, estimates the target's scale change. This is
because we observe the confidence map from the scale filter, $R_s$ is
sometime noisy. In addition, at every iteration, we check the variance
in the velocity components of the particles to re-assign velocities
from uniform initial distribution to keep variance high enough.
\end{comment}
%----------------------------------------------------------------------
\section{Experiments} \label{sc:Experiments}
%---------------------------------------------------------------------- 
To verify the usefulness of our algorithm, we choose two public,
available datasets:
OTB100 \footnote{\url{http://cvlab.hanyang.ac.kr/tracker_benchmark/benchmark_v10.html}}
and
UAV123 \footnote{\url{https://ivul.kaust.edu.sa/Pages/Dataset-UAV123.aspx}}\cite{mueller2016uav123}.
The OTB100 dataset is comprised of the videos about 100 objects
whereas UAV123 dataset contains aerial footage of 123 objects. We aims
at developing an object tracking algorithm that ``efficiently'' runs
in ``high speed,'' $\ge 300$ fps. Given this goal, we believe that the
UAV123 video data is a better fit in evaluating the performance of the
proposed algorithm. This is because there are many challenging scenes,
e.g., abrupt scale/illumination changes as all the targets were
recorded from bird-eye view. Although we are primarily interested in
developing an algorithm for mobile, embedded robotic applications, it
is important to compare the performance of the proposed algorithm with
those of the state-of-the-art in nominal dataset for comparison. To
this end, we chose the OTB100 data that contains the videos recorded
from smart phones and typical cameras in perspective view. Finally,
the {\it E}nKCF2 is tested on temporally down-sampled version of the
UAV123 dataset which is named UAV123$\_$10fps dataset.

\textbf{Finding Optimal Hyperparameters} Each of three KCFs in {\it
  E}nKCF is designed for addressing a specific challenge of a single,
target tracking problem. Thus each of them should have different value
for the optimal parameters. For instance, the primary task of the
translation filter, $R_{t}^{L}$ is to estimate the translation of a
target from the previous frame whereas the scale filter, $R_{s}$ and
the estimated state of the particle filter are primarily used to
estimate the scale and translation. Because of this, we set the
learning rates ($\beta$) of individual filters, $R_{t}^{L}$,
$R_{t}^{S}$, and $R_{s}$ differently as $0.020$, $0.020$ and
$0.010$. For the kernel of the Gaussian neighboring function, we
empirically found the optimal values of $\alpha$ as $0.6$, $0.4$, and
$0.4$ for $R_{t}^{L}$, $R_{t}^{S}$, and $R_{s}$, and the learning rate
as $0.12$ for $R_{t}^{L}$. For the particle filter, we empirically set
the number of particles to $1000$ to balance the run-time performance
and accuracy. To keep the level of variance among the particles
reasonable, we performed the re-sampling only when the
\textit{efficient number of samples} ($ \hat{N}_{eff} \approx
(\sum_{p=1}^{P}w_{p}^{2})^{-1} $) is lower than a pre-defined
threshold. 

\textbf{Features Selections} As each of three KCFs in the {\it E}nKCF
has different goals to accomplish, we used different features for each
of them. In particular, the translation filter, $R_{t}^{L}$, uses both
the fast Histograms of Oriented Gradients
(fHoG)\cite{felzenszwalb2010object} and
color-naming\cite{van2009learning} features to learn the background
around the target more effectively. In fact we empirically found fHoG
only didn't work well to cover the larger ROI. Another translation
filter, $R_{t}^{S}$, used fHoG features only. The scale filter,
$R_{s}$ used both fHoG and color features to learn the scale change
more accurately.

\textbf{Performance on UAV123 Dataset} We compared the performance of
the proposed tracking algorithm with those of the state-of-the-art
high speed tracking algorithms in the \textit{Precision} and
\textit{Success\:Rate} metrics. In precision, we rank the trackers
based on the precision numbers at 20 pixels whereas in the success
rate plots, trackers are ranked based on area under curve (AUC)
scores. The tracking algorithms used for the comparison include ones
in high-speed ($\geq$300 fps), i.e., KCF\cite{henriques2015high}, CSK
\cite{henriques2012exploiting}, DCF\cite{henriques2015high}, and
MOSSE\cite{bolme2010visual,henriques2015high} and ones in relatively
lower-speed ($\geq$50), i.e., SAMF\cite{li2014scale},
DSST\cite{danelljan2014accurate}, Struck\cite{hare2012efficient},
MUSTER \cite{hong2015multi}. TLD \cite{kalal2012tracking}, and OAB
\cite{zhang2012robust}. Figure \ref{fig:UAV123_OTB100_DATASET} shows
the results on the UAV123 dataset. {\it E}nKCF outperformed most of
the high-speed tracking methods by $3\%$-$15\%$ at 20 pixels precision
metric. Three algorithms, SAMF, DSST, and Struck did about 5\% better
than ours in accuracy, but 10-20\% slower than ours. For the scale
adaptation, {\it E}nKCF did second best in terms of \textit{area under
  curve} (AUC) value for the success rate plot. It outperformed other
tracking methods in high-speed by about $20\%$-$25\%$ in AUC
metric. In addition, it performed even better than those algorithms
that are slow, but a bit better in precision. For example, for AUC,
{\it E}nKCF outperformed Struct and DSST by $5\%$ and $10\%$ while
running at more than $10$ and $30$ times faster.

\textbf{Performance on OTB100 Dataset} We evaluated the performance of
the {\it E}nKCF tracker on the OTB100 dataset. Figure
\ref{fig:UAV123_OTB100_DATASET} shows the results on the OTB100
dataset. Like in UAV123 dataset, we observed a similar performance. In
particular, {\it E}nKCF showed a good performance for handling the
scale changes. It was the third best and the second fastest tracking
algorithm. For the precision metric, {\it E}nKCF performed slightly
better than most of tracking algorithms in high-speed
trackers. Interestingly, it outperformed another correlation filter
based tracker, DSST, but performing $5\%$ behind of another low-speed
scale adaptive SAMF tracker.
\begin{figure}[!h]
\centering
\begin{tabular}{ccc}
\bmvaHangBox{\includegraphics[width=2.80cm]{./figures/Precision_UAV123.eps}}
\bmvaHangBox{\includegraphics[width=3.30cm]{./figures/Success_UAV123.eps}}
\bmvaHangBox{\includegraphics[width=2.8cm]{./figures/Precision_OTB100.eps}}
\bmvaHangBox{\includegraphics[width=3.30cm]{./figures/Success_OTB100.eps}}\\
\centering \footnotesize (a) Precision and Success rates for UAV123 \quad \quad \quad \quad \quad (b) Precision and Success rates for OTB100
\end{tabular}
\caption{Comparison of the slightly, modified version of {\it E}nKCF
  for the UAV123 (a) and OTB100 (b) datasets.}
\label{fig:UAV123_OTB100_DATASET}
\end{figure}

\textbf{Performance on UAV123$\_$10fps Dataset.} It maybe a slight
digression from the main idea -- the idea of deploying multiple KCFs
to make a very fast and effective, single target tracking
available. We found that, with a slight modification, the proposed
method can effectively handle tracking of object in fast motion and in
large camera motion. Specifically we modified {\it E}nKCF by running
$R_{t}^{L}$ every frame and remove the particle filter instead. We
tested this modified {\it E}nKCF with another vision of the UAV123 that
the original dataset was temporally down-sampled at $10$ fps and makes
the challenging original dataset, because of objects in fast motion
and large ego-motion, more challenging. This is because the magnitude
of those motions is even bigger. Figure \ref{fig:UAV123_10fpsDATASET}
compares the performance of the modified version of {\it E}nKCF and
other tracking methods. Our tracker outperforms other high-speed
correlation filter based trackers including KCF, DSST by about
$15\%$-$20\%$ and showed a precision rate similar to that of SAMF. On
the other hand, it does decent job in scale estimation. It ranks as
second in AUC while running about $150$ faster than the MUSTER
tracker.
\begin{figure}[!h]
\centering
\begin{tabular}{cc}
\bmvaHangBox{\includegraphics[width=3.30cm]{./figures/Precision_UAV123_10fps.eps}}
\bmvaHangBox{\includegraphics[width=3.30cm]{./figures/Success_UAV123_10fps.eps}}\\
\end{tabular}
\caption{Evaluation and comparison of the proposed {\it E}nKCF2 tracker on the UAV123$\_$ dataset in terms of the precision and success rates. This dataset consists of temporally downsampled $123$ video sequences captured from a micro UAV.}
\label{fig:UAV123_10fpsDATASET}
\end{figure}

%\begin{figure}[!h]
%\centering
%\begin{tabular}{cc}
%\bmvaHangBox{\includegraphics[width=2.50cm]{./figures/Precision_OTB100.pdf}}
%\bmvaHangBox{\includegraphics[width=2.50cm]{./figures/Success_OTB100.pdf}}\\
%\end{tabular}
%\caption{Evaluation and comparison of the proposed {\it E}nKCF tracker on the OTB100 dataset. OTB100 dataset consists of $100$ video sequences of different objects from recent literatures.}
%\label{fig:OTB100_DATASET}
%\end{figure}
%---------------------------------------------------------------------- 
\section{Conclusion} \label{sc:Conclusion}
%---------------------------------------------------------------------- 
Running a computer vision algorithm on any existing embedded systems
for real-world applications is economically and practically very
attractive. To make such pipelines more plausible, however, those
algorithms should run faster with small footprint of computation
resource consumption. To make an object tracking algorithm with these
practical requirements available, we proposed an extension of KCF that
essentially deploys three KCFs, in turn, to address the scale change
and dynamic maneuvers of a target. The way we ran three KCFs is
optimal in a sense that efficiently handled the scale change and
effectively learned the translation of the target. We also developed a
Bayes filter, particle filter, to smooth the transition between three
KCFs. In particular, the particle filter estimated the target's pixel
coordinates using information about the target's motion observed over
frames. We used two publicly available dataset to evaluate the
performance of the proposed algorithm and found, on average, the
performance of the proposed algorithm is better than those of the
existing ones over 5\% on precision at 20 pixels and 10-20\% on AUC on
average. Our implementation ran at 340 fps for OTB100 and at 416 fps
for UAV123 data that is faster than DCF (292 fps) for OTB100 and KCF
(292 fps), DCF (457 fps) for UAV123.

As future work, we investigates a way of re-initializing a target when
the target is lost, due to occlusion, illumination change, drastic
camera motion, etc.
%--------------
%\section*{Acknowledgements}
%put stuff here for the accepted , but not the ICCV version

\bibliography{draft}
\end{document}
